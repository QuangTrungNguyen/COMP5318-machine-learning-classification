{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment1.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1mwRFOIr1cH9GBPxzjsS4jzz007MUX_Sm","timestamp":1525522413863}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"xSLnEudzPmgt","colab_type":"text"},"cell_type":"markdown","source":["# COMP5318 - Machine Learning and Data Mining - S1 2018"]},{"metadata":{"id":"QSeeeotjP54F","colab_type":"text"},"cell_type":"markdown","source":["## Assignment 1 - Due 07 May 2018, 5:00pm"]},{"metadata":{"id":"fIvhv-pSQZPL","colab_type":"text"},"cell_type":"markdown","source":["Please refer to [Description File](https://drive.google.com/open?id=1fUFTqcjbSR75tiS48imwjlEVxZVS6U42YElwoVPexwY) for information about the assignment.\n","\n"]},{"metadata":{"id":"KcYZJPr8frtm","colab_type":"text"},"cell_type":"markdown","source":["#### Group Number: 75\n","#### Group Members (name and student number):\n","\n","*   Quang Trung Nguyen - 470518197\n","*   Mingxuan Li - 470325230\n","*   Yukui Chen - 470183984\n","\n","#### Tutor Names: \n","*   Kelvin & Anthony\n","*   Harrison Nguyen\n","\n","#### Instructions to run the code:\n","*   Please click 'Run all' in the 'Runtime' if you want to run them all in sequence, or you can run each code cell in order. Please let us know if you have any questions about running the codes in the codelab.\n"]},{"metadata":{"id":"nOWmWCq3gEwF","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n"]},{"metadata":{"id":"Dsk4FUnOUx3s","colab_type":"text"},"cell_type":"markdown","source":["## Authenticate and create PyDrive client."]},{"metadata":{"id":"OcFUG6SMVzkY","colab_type":"text"},"cell_type":"markdown","source":["You will be prompted with a link to click on, and give permission to Google Colab to access your Google Drive. If you don't want to give permission to your personal google drive, create a new gmail account, and complete this process using the new account."]},{"metadata":{"id":"Z_FCt_RtAnpE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# PyDrive reference:\n","# https://googledrive.github.io/PyDrive/docs/build/html/index.html\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eaAbndQYVTKB","colab_type":"text"},"cell_type":"markdown","source":["## Import data\n","Locate the \"Data\" folder in your drive. Right click and click \"share\" to get the ID of the folder. Replace < Data folder id > with the id you got. (id should look like \"1j8oG_vCmum965Ghg8LdbSkfj-lfi-AZ0\" )"]},{"metadata":{"id":"oknGSWjHNCmO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import os\n","import psutil\n","import numpy as np\n","import pandas as pd\n","import scipy.sparse as sparse\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X9Ep4s9BeoKp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Define Helper Function\n","### Reference: http://fa.bianp.net/blog/2013/different-ways-to-get-memory-consumption-or-lessons-learned-from-memory_profiler/\n","def memory_usage_psutil():\n","    # return the memory usage in MB\n","    process = psutil.Process(os.getpid())\n","    mem = process.memory_info().rss / float(2 ** 20)\n","    return mem"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A2siYFZuM2JJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":85},"outputId":"a384b05d-fe00-4683-d71f-47ea99325efd","executionInfo":{"status":"ok","timestamp":1525658666789,"user_tz":-600,"elapsed":2306,"user":{"displayName":"Leslie CHEN","photoUrl":"//lh4.googleusercontent.com/-FXnDbbiS1ww/AAAAAAAAAAI/AAAAAAAAAY4/srCxMVswUdw/s50-c-k-no/photo.jpg","userId":"113849209760449392369"}}},"cell_type":"code","source":["file_list = drive.ListFile({'q': \"'1j8oG_vCmum9YuVWP8LdbSkfj-lfi-AZ0' in parents and trashed=false\"}).GetList()\n","for file1 in file_list:\n","  print('title: %s, id: %s' % (file1['title'], file1['id']))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["title: training_desc.csv, id: 1kSoeoaK_TsTHnhSGPXOLNy43jd20G8bH\n","title: training_data.csv, id: 12-B9GpferXatOB4ACo7GanQ2iBt5O-66\n","title: test_data.csv, id: 1y_YySFyibYYwQtJr6v7Qf81FqULwK7Fk\n","title: training_labels.csv, id: 1cqErOBjBB91P_xrt4TKuZ91OfTLoUYmx\n"],"name":"stdout"}]},{"metadata":{"id":"iGOTPZQWWlf8","colab_type":"text"},"cell_type":"markdown","source":["### Pulling data into Google Colab."]},{"metadata":{"id":"vo17S6sITJfa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["training_data_downloaded = drive.CreateFile({'id': '12-B9GpferXatOB4ACo7GanQ2iBt5O-66'})\n","training_data_downloaded.GetContentFile('training_data.csv')\n","\n","training_desc_downloaded = drive.CreateFile({'id': '1kSoeoaK_TsTHnhSGPXOLNy43jd20G8bH'})\n","training_desc_downloaded.GetContentFile('training_desc.csv')\n","\n","training_labels_downloaded = drive.CreateFile({'id': '1cqErOBjBB91P_xrt4TKuZ91OfTLoUYmx'})\n","training_labels_downloaded.GetContentFile('training_labels.csv')\n","\n","test_data_downloaded = drive.CreateFile({'id': '1y_YySFyibYYwQtJr6v7Qf81FqULwK7Fk'})\n","test_data_downloaded.GetContentFile('test_data.csv') "],"execution_count":0,"outputs":[]},{"metadata":{"id":"QhI6TXRtWsl7","colab_type":"text"},"cell_type":"markdown","source":["### Load data files\n","(example:)"]},{"metadata":{"id":"l2SIs1Kqe5Xz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Define Data Dimensions\n","train_data_dimension = np.array([20104, 13627])\n","test_data_dimension = np.array([2233, 13627])\n","usecolumns = np.arange(1, train_data_dimension[1], 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"saVLVnGvT59I","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Load Files\n","### Reference - Read CSV using Pandas: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n","### Reference - Load Big Data using Pandas: https://www.dataquest.io/blog/pandas-big-data/\n","df_train_names = pd.read_csv('training_data.csv', sep=',', engine='c', usecols=[0], header=None, names=['app_name'])\n","df_train_data = pd.read_csv('training_data.csv', sep=',', engine='c', dtype=np.float32, usecols=usecolumns, header=None)\n","df_test_names = pd.read_csv('test_data.csv', sep=',', engine='c', usecols=[0], header=None, names=['app_name'])\n","df_test_data = pd.read_csv('test_data.csv', sep=',', engine='c', dtype=np.float32, usecols=usecolumns, header=None)\n","df_train_labels = pd.read_csv('training_labels.csv', sep=',', engine='c', header=None, names=['app_name', 'app_label'])\n","# df_desc = pd.read_csv('training_desc.csv', sep=',', engine='c', header=None)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X_9yMelvf_GK","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n"]},{"metadata":{"id":"O5zfbjqlgSWN","colab_type":"text"},"cell_type":"markdown","source":["## 1. Introduction (max 500 words)\n"]},{"metadata":{"id":"N6KtBHrPgiCB","colab_type":"text"},"cell_type":"markdown","source":["### 1.1 What is the aim of the study?\n"]},{"metadata":{"id":"WP6x1GfHgrX1","colab_type":"text"},"cell_type":"markdown","source":["\n","We have three aims for this study. The first aim is to find an effective classification method that can be applied to the problem of app classification and to build an appropriate classifier to tag apps with good accuracy and minimal running time into the correct label based on the app’s description. The dataset introduces TF-IDF values to represent the importance of a word in an App's description. As there are 13,626 unique words which correspond to their frequency in the description, we treat them as 13,626 features. Therefore, the classification method we will choose should be suitable for dealing with high-dimensioned data so that it can run within feasible time.\n","\n","</br>\n","\n","The second aim of this study is to learn about the working principle of the Naive Bayesian classifier which is the approach we choose to solve the problem of app classification. The Naive Bayesian approach is good at text classification. It is a probabilistic classifier with the assumption of independency among features, and this assumption is helpful when the dimensionality of the input data is high (Bishop, 2006). As the number of feature dimensions increases, the total number of possible feature vectors increases exponentially. This severe difficulty that may arise from moving to spaces of higher dimension was termed the curse of dimensionality by Richard Bellman (1961). Text classification is such a domain with a large number of features (McCallum and Nigam 1998).  In this case, we will consider Naive Bayes classifier which assumes that all attributes of the data are independent of each other given the context of the class.\n","\n","</br>\n","\n","The third aim of this study is to judge and criticize the classifier we build in terms of accuracy, running time, memory usage and ultimately evaluate the applicability of Naïve Bays approach in this problem. As limitation are a part of the scientific study, we provide an explanation for each limitation, showing why the classification results are important.\n","\n","\n"]},{"metadata":{"id":"GRYT8q82g1uv","colab_type":"text"},"cell_type":"markdown","source":["###1.2 Why is this study important?"]},{"metadata":{"id":"W03lryJrg-EX","colab_type":"text"},"cell_type":"markdown","source":["There are some reasons that drive our team to undertake this classification task. \n","Firstly, in this particular case of App classification, more classification tasks in App market can be efficiently processed by a machine learning based classifier which simply learns from training data of the App's TD-IDF value. As the widespread and increasing availability of mobile Apps in the Apps Markets presents significant challenges to sort the Apps into an appropriate category, in order to make it easier for Apps users to search and download the apps, developing a better classifier powered by machine learning algorithm helps the market reduce cost and save more time for the app development community and ultimately enhances user experience.\n","\n","</br>\n","Moreover, in general, an effective classification method allows efficient study in many fields, for both commercial and academic purposes. For example, in biological study, if the organisms can be categorized based on their characteristics, ancestry and so on, the researchers will have the ability to conduct deeper learning on them and even possibly extract some meaningful insights and patterns which almost impossible to recognize without an effective classification.\n","\n","\n"]},{"metadata":{"id":"vXjExeGHhfdL","colab_type":"text"},"cell_type":"markdown","source":["## 2. Methods "]},{"metadata":{"id":"l78VSnashn5y","colab_type":"text"},"cell_type":"markdown","source":["### 2.1 Pre-processing\n","\n","\n","---\n","\n"]},{"metadata":{"id":"UdWHc8SLh3m7","colab_type":"text"},"cell_type":"markdown","source":["The pre-processing before we build the classifier includes three parts which are (1) Load data efficiently and using less memory, (2) Construct training data and testing data into a sparse matrix and (3) Join the training data and training labels in the same orders by the app_name. Firstly, we use pandas to read the csv file into dataframes types, we use C engine to achieve faster data loading, we also load the training data into ***df_train_names*** and ***df_train_data*** separately, so ***df_train_names*** are strings and will be used in joining with training labels in same order in step 3 as mentioned above. The actual training data and testing data are loaded in *np.float32* type to save 50% more memory while loading as by default *np.float64*.\n","\n","</br>\n","\n","After loading the data of csv file into dataframes, we use *np.nonzero* function to find all the non-zero indices in the training and testing matrix, then we use spicy sparse matrix library and those non-zero indices to construct two sparse matrices (***sp_matrix_train*** and ***sp_matrix_test***), then we delete those dataframes from step 1 to release those memories for garbage collection (GC) to collect at later stage. Once the training and testing sparse matrices is constructed, we join the labels into the same order as the app_names (first column) in the training data set so we can map the labels into the training data properly for later classifiers and predictions steps.\n","\n","\n","</br>\n","\n","\n","The pre-processing steps help us save much more memories. In the local environment we use around 3GB without pre-processing, while using pre-processing we only use around 500MB. Similarly, in google colab platform, we actually spend *4649.234375MB* in total by using preprocessing steps as described above without affecting the overall runtime and accessibilities of the sparse matrices.\n"]},{"metadata":{"id":"oWvHtQ4Shlmr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":51},"outputId":"a8208331-fe2b-4b03-9258-c1c1692b2294","executionInfo":{"status":"ok","timestamp":1525658780635,"user_tz":-600,"elapsed":8817,"user":{"displayName":"Leslie CHEN","photoUrl":"//lh4.googleusercontent.com/-FXnDbbiS1ww/AAAAAAAAAAI/AAAAAAAAAY4/srCxMVswUdw/s50-c-k-no/photo.jpg","userId":"113849209760449392369"}}},"cell_type":"code","source":["### Reshape the data\n","### Convert DataFrame to Matrix\n","train_data = df_train_data.astype(np.float32).as_matrix()\n","test_data = df_test_data.astype(np.float32).as_matrix()\n","\n","### Compress Sparse Matrix\n","sp_train_indices = np.nonzero(train_data)\n","sp_train_data = train_data[sp_train_indices].astype(np.float32)\n","sp_test_indices = np.nonzero(test_data)\n","sp_test_data = test_data[sp_test_indices].astype(np.float32)\n","\n","sp_matrix_train = sparse.csc_matrix((sp_train_data, (sp_train_indices[0], sp_train_indices[1])), shape=df_train_data.shape)\n","sp_matrix_test = sparse.csc_matrix((sp_test_data, (sp_test_indices[0], sp_test_indices[1])), shape=df_test_data.shape)\n","print(sp_matrix_train.shape)\n","print(sp_matrix_test.shape)\n","\n","### Delete those dataframes to give to gc to recycle memory as we dont need them for classifier and data analysis below\n","del df_train_data\n","del df_test_data\n","del train_data\n","del test_data\n","\n","df_train_mapped_names_labels = df_train_names.join(df_train_labels.set_index('app_name'), on='app_name')\n","df_train_unique_labels = df_train_mapped_names_labels.app_label.unique()\n","df_train_unique_labels_size = df_train_unique_labels.size"],"execution_count":32,"outputs":[{"output_type":"stream","text":["(20104, 13626)\n","(2233, 13626)\n"],"name":"stdout"}]},{"metadata":{"id":"P1_hsOaLhfTf","colab_type":"text"},"cell_type":"markdown","source":["### 2.2 Classifier"]},{"metadata":{"id":"pnvIjGhUh6Mp","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n"]},{"metadata":{"id":"Os1-qbDXeH61","colab_type":"text"},"cell_type":"markdown","source":["aAccording to McCallum and Nigam (1998), text classification is such a domain with a large number of features. In this project, word frequency in app’s description can be treated as the word counts in text and the fractional counts TF-IDF value can be seen as integer feature counts. By consider Naive Bayes classifier, we assume that all attributes of the data are independent of each other given the context of the class. Besides a large number of features, we need to consider other factors like easy and fast implementation. The training time will be a big concern because our training data consists 20,104 apps with 13,626 features and the task are to classify them into 30 unique labels. This suggests Naive Bayes classifier normally to be a good technique used as a baseline in text classification. Naive Bayes classifier requires less model training time due to its simplicity especially for the classification tasks of text data with high dimensions. According to Statista (2017), the current growth rate of mobile apps in the market is more than 1,300 apps per day, which means the input data in Apps Markets are dynamic, Naive Bayes classifier can adapt quickly to the changing data due to its simple and fast algorithm. \n","\n","</br>\n"," \n","Under the Bayesian probabilistic framework, Naive Bayes approach uses the training data which is assumed to be generated by a parametric model to compute Bayes-optimal estimates of the model parameters (McCallum and Nigam 1998). With the classifier that is produced by the training data, we can use it to classify new test data via Bayes’ rule to turn the generative model around and compute the posterior probability that a class will generate on the test examples. Then, any new app will be tagged into the most probable label at a certain accuracy that our classifier suggests.\n","\n","</br>\n","\n","Based on Bayes' theorem, to predict class C, specifically, we need to find the value of C that maximizes  $P(C | A1, A2,..., An )$. As we want to calculate the posterior probability $P(C | A1, A2, ..., An)$ for all values of C using the Bayes' theorem, which is equivalent to select value of C that maximises $P(A1, A2, ..., An | C) P(C) $. We will use Naive Bayes classifier to estimate $P(A1, A2, ..., An | C )$. \n","\n","</br>\n","\n","$$ P(C \\mid A) = \\frac{P(A \\mid C) \\, P(C)}{P(A)} $$\n","\n","</br>\n","\n","In Naive Bayes approach, we assume the feature probabilities $P(A_i|C_j)$ are independent given the class C.\n","\n","</br>\n","\n","$$ P({A_1,A_2...A_n} \\mid C) = P(A_1 \\mid C) \\times P( A_2 \\mid C)\\times...\\times P( A_n \\mid C) $$\n","\n","</br>\n","\n","The Naïve Bayes classifier (NBC) is given by,\n","\\begin{equation}\n","    C_{NB} = \\text{argmax}_{k \\in {1,2,...K}} \\Big( p(C_k) \\prod_{i=1}^D p(x_i|C_k) \\Big) \\in \\{C_k\\}_{k=1}^K\n","\\end{equation}\n","\n","where $C_k$ is the $30^{th}$ class when there are $30$ classes. \n","\n","</br>\n","\n","The general Naive Bayes approach refers strong independence assumptions in the model, instead of the particular distribution of each feature. By considering the fact that the TF-IDF value, which is used to weigh each word in the app's description according to its uniqueness, is essentially a calculation of a discrete feature to present the word counts, we will implement the Multinomial Naive Bayes model for classification. Russel and Norvig (2003) also supported that multinomial Naive Bayes works well for data which can easily be turned into counts, such as word frequency in text. To be more specifically, Multinomial Naïve Bayes classifier is a special form of a Naïve Bayes classifier that assumes the multinomial distribution for each feature in the model. \n","\n","\n","\\begin{align}\n","\\log p(C_k \\mid \\mathbf{x}) & \\varpropto \\log \\left( p(C_k) \\prod_{i=1}^n {p_{ki}}^{x_i} \\right) = \\log p(C_k) + \\sum_{i=1}^n x_i \\cdot \\log p_{ki}\n","\\end{align}\n","\n","\n","</br>\n","\n","\n","The multinomial naive Bayes classifier is basically a linear classifier when represented in log-space. Therefore, we take the log of each probability of each class then sum them up to get the log form of posterior probability of the class.\n","</br></br>\n","\n","To implement the model, we defined two functions, one for calculating the feature probabilities, and the other for predicting the class probabilities given the features. The first function ***multinomial_Bayes_classifier*** takes in three parameters: train data, train labels and feature count. For each class label *k*, we first retrieve all rows in the train data belonging to class *k*, then calculate the log-probability $Pki$ of each feature *i* given that class *k*, store the values in a matrix and return it to the caller. The next function ***predict_labels*** takes in four parameters: test data, class label count, feature probabilities, and class probabilities. This function calculates the log-probability $log p(C_k \\mid \\mathbf{x})$ for each class *k* given the set of features in the test data using  $\\log p(C_k) + \\sum_{i=1}^n x_i \\cdot \\log p_{ki}$ where $x_i$ is the TF-IDF values in the test data, then select the class with the highest log-probability to be the predicted label."]},{"metadata":{"id":"HlaepfeQSvPo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### MULTINOMAL NAIVE BAYES MODEL \n","\n","def multinomial_Bayes_classifier(train_data, x_train_labels, feature_count):\n","    x_train_labels_counts = x_train_labels.value_counts()\n","    feature_probabilities = np.empty((0, feature_count), dtype=np.float32)\n","    # For each class label, calculate each feature probability given that class\n","    for labelname, count in x_train_labels_counts.items():\n","        # Get the data of each class\n","        class_rows_indices = x_train_labels[x_train_labels == labelname].index\n","        class_data = train_data[class_rows_indices, :]\n","        # Calculate feature probabilities \n","        feature_prob = np.log(class_data.sum(axis=0) + 1)\n","        feature_probabilities = np.append(feature_probabilities, feature_prob[0], axis=0)\n","    return feature_probabilities\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cuQCq8BnYYjm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Function to Predict Labels\n","\n","def predict_labels(test_data, feature_probabilities, x_train_labels_counts, x_train_class_probabilities):\n","    fold_size = test_data.shape[0]\n","    # Reshape class probablities into a matrix\n","    x_train_class_probabilities_matrix = x_train_class_probabilities.repeat(fold_size, axis=0).reshape(fold_size, x_train_labels_counts.size)\n","    # Calculate class probabilities given feature set and select the class with the highest probability\n","    predicted_label_indices = np.argmax(test_data * feature_probabilities.T + x_train_class_probabilities_matrix, axis=1)\n","    predicted_results = []\n","    # Convert numerical class label into respective text label\n","    for labelindex in np.squeeze(np.asarray(predicted_label_indices)):\n","        predicted_results.append(x_train_labels_counts.keys().values[labelindex])\n","    return np.asarray(predicted_results)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ev9zV335hv0m","colab_type":"text"},"cell_type":"markdown","source":["## 3. Experiments and Results "]},{"metadata":{"id":"GrOvQk1krsRP","colab_type":"text"},"cell_type":"markdown","source":["We defined two functions, ***calculate_performance_metrics*** for calculating the performance measures on the result of our classifier, and ***k_fold_cross_validation*** for dividing the dataset into k folds for cross validation. The ratio used is 9:1 for train and test sets in each iteration.\n","</br> </br>\n","We measured the performance of our classifier not only by accuracy but also using precision, recall, f-score. We calculated these three performance metrics for each class as we do with binary classification, then we averaged them for the entire dataset."]},{"metadata":{"id":"Gmt8XsbnYrlU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Function to calculate performnance metrics\n","\n","def calculate_performance_metrics(y_true, y_pred):\n","  TP = [] # True Positive\n","  TN = [] # True Negative\n","  FP = [] # False Positive\n","  FN = [] # True Negative\n","  \n","  # Calculate the precision, recall, f-measure for each class label\n","  for label in df_train_unique_labels:\n","    true = np.copy(y_true)\n","    pred = np.copy(y_pred)\n","    # Relabel the data as binary classification \n","    true[true != label] = 'none'\n","    pred[pred != label] = 'none'\n","    TN.append(len(np.where((pred != label) & (true == pred))[0]))\n","    TP.append(len(np.where((pred == label) & (true == pred))[0]))\n","    FP.append(len(np.where((pred == label) & (true != pred))[0]))\n","    FN.append(len(np.where((pred != label) & (true != pred))[0]))\n","    \n","  # Precision = TP/(TP+FP)\n","  precision = np.asarray(TP) / ( np.asarray(TP) + np.asarray(FP)) \n","  # Recall = TP/(TP+FN)\n","  recall = np.asarray(TP) / ( np.asarray(TP) + np.asarray(FN) ) \n","  # F-measure = 2TP/(2TP+FN+FP)\n","  f_measure = 2*np.asarray(TP) / ( 2*np.asarray(TP) + np.asarray(FN) + np.asarray(FP) )\n","  \n","  # Return the mean scores across all classes\n","  return np.mean(precision),np.mean(recall),np.mean(f_measure)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bsrMY2bTYx5e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### Functions for k-fold cross validation\n","\n","def k_fold_cross_validation(train_data, train_mapped_names_labels, k):\n","    sample_size = train_data.shape[0]\n","    feature_count = train_data.shape[1]\n","    fold_size = int(sample_size / k)\n","    # Performance metrics\n","    accuracies = []\n","    precisions = []\n","    recalls = []\n","    f_measures = []\n","    \n","    ### Divide the dataset into k folds\n","    for start in np.arange(0, sample_size, fold_size):\n","        stop = start + fold_size\n","        if stop > sample_size:\n","            break\n","\n","        ### For each fold we find the corresponding training data set and testing data set with ratio (9:1)\n","        x_test_indices = np.arange(start, stop, 1)\n","        x_test_data = train_data[x_test_indices]\n","        x_test_labels = train_mapped_names_labels['app_label'][x_test_indices]\n","        x_train_indices = np.append(np.arange(0, start, 1), np.arange(stop, sample_size, 1))\n","        x_train_data = train_data[x_train_indices]\n","        x_train_labels = train_mapped_names_labels['app_label'][x_train_indices]\n","        x_train_labels_counts = x_train_labels.value_counts()\n","        \n","        # Calculate class probabilities (priors) from the train data\n","        x_train_class_probabilities = np.log(x_train_labels_counts / x_train_labels.size).values\n","        # Fit the train data to calculate feature probabilities\n","        feature_probs = multinomial_Bayes_classifier(train_data, x_train_labels, feature_count)    \n","        # Predict labels for test data \n","        predicted_result = predict_labels(x_test_data, feature_probs, x_train_labels_counts, x_train_class_probabilities) \n","        \n","        # Calculate accuracy, performance metrics\n","        accuracies.append(sum(np.asarray(x_test_labels) == predicted_result) / fold_size)\n","        precision,recall,f_measure = calculate_performance_metrics(np.asarray(x_test_labels),predicted_result)\n","        precisions.append(precision)\n","        recalls.append(recall)\n","        f_measures.append(f_measure)\n","        \n","    # Calculate and print the mean, standard deviation of each metric\n","    print('Mean Accuracy: {:0.2f}%'.format(np.mean(np.asarray(accuracies))*100))\n","    print('Accuracy Std Dev: {:0.2f}%'.format(np.std(np.asarray(accuracies))*100))\n","    print(accuracies)\n","    print('Mean Precision: {:0.3f}'.format(np.mean(np.asarray(precisions))))\n","    print('Precision Std Dev: {:0.3f}'.format(np.std(np.asarray(precisions))))\n","    print(precisions)\n","    print('Mean Recall: {:0.3f}'.format(np.mean(np.asarray(recalls))))\n","    print('Recall Std Dev: {:0.3f}'.format(np.std(np.asarray(recalls))))\n","    print(recalls)\n","    print('Mean F-measure: {:0.3f}'.format(np.mean(np.asarray(f_measures))))\n","    print('F-measure Std Dev: {:0.3f}'.format(np.std(np.asarray(f_measures))))\n","    print(f_measures)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hWFnjiN2iG1M","colab_type":"text"},"cell_type":"markdown","source":["### 3.1 Accuracy"]},{"metadata":{"id":"2ggc68nniKM8","colab_type":"text"},"cell_type":"markdown","source":["We performed 10-fold cross validation and take average of the accuracy across 10 iterations and finally get a mean accuracy of 63.60%. The standard deviation of the accuracies is very low - 0.67%. This indicates there are only small differences in the accuracies over 10 iterations, and the mean average is a good representation of the accuracy over the entire dataset. "]},{"metadata":{"id":"5KmSfVDAth1y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":258},"outputId":"9a97d90e-54be-4c1f-9dee-c41925bc4346","executionInfo":{"status":"ok","timestamp":1525658886784,"user_tz":-600,"elapsed":84837,"user":{"displayName":"Leslie CHEN","photoUrl":"//lh4.googleusercontent.com/-FXnDbbiS1ww/AAAAAAAAAAI/AAAAAAAAAY4/srCxMVswUdw/s50-c-k-no/photo.jpg","userId":"113849209760449392369"}}},"cell_type":"code","source":["# Timing\n","import timeit\n","start = timeit.default_timer()\n","\n","# Perform 10-fold cross validation\n","# This should take roughly 1.5 minutes\n","k = 10\n","k_fold_cross_validation(sp_matrix_train, df_train_mapped_names_labels, k)\n","\n","stop = timeit.default_timer()\n","print ('Runtime: {:0.2f} seconds'.format(stop - start ))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Mean Accuracy: 63.60%\n","Accuracy Std Dev: 0.67%\n","[0.6437810945273632, 0.6308457711442786, 0.6358208955223881, 0.627363184079602, 0.6318407960199005, 0.6388059701492538, 0.6373134328358209, 0.6497512437810945, 0.6278606965174129, 0.6368159203980099]\n","Mean Precision: 0.660\n","Precision Std Dev: 0.011\n","[0.6765124115507664, 0.6493054171874849, 0.6629803664475552, 0.650369218983579, 0.6544886671532452, 0.6615587074556031, 0.6628238056706273, 0.6802343660864627, 0.6418735824328974, 0.6553218255196313]\n","Mean Recall: 0.625\n","Recall Std Dev: 0.005\n","[0.6315976337658676, 0.6189976877408248, 0.624350648170524, 0.6214385055806506, 0.6202311268591459, 0.6243804255720945, 0.6244866700573405, 0.6313506045939948, 0.6201518215703296, 0.6332611325052198]\n","Mean F-measure: 0.618\n","F-measure Std Dev: 0.006\n","[0.6255821136543153, 0.6138708372930722, 0.6172074561103998, 0.6128873423635759, 0.612648718083106, 0.6184662872032415, 0.618521349418539, 0.6306453301073985, 0.6080642560802045, 0.6224000691023821]\n","Runtime: 83.74 seconds\n"],"name":"stdout"}]},{"metadata":{"id":"rIKV_buufnSY","colab_type":"text"},"cell_type":"markdown","source":["We then compared the result of our classifier with scikit-learn's Multinomial Naive Bayes classifier and its k-fold function. The accuracy from their classifier using 10-fold cross validation on our train data is very similar to the accuracy of our own classifier (63.23% and 63.60% respectively)."]},{"metadata":{"id":"QgeFYT9CfmPH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"3dd281e8-fe07-412a-e5a4-2b310788f617","executionInfo":{"status":"ok","timestamp":1525658893250,"user_tz":-600,"elapsed":3176,"user":{"displayName":"Leslie CHEN","photoUrl":"//lh4.googleusercontent.com/-FXnDbbiS1ww/AAAAAAAAAAI/AAAAAAAAAY4/srCxMVswUdw/s50-c-k-no/photo.jpg","userId":"113849209760449392369"}}},"cell_type":"code","source":["# Test our data with scikit-learn MultinomialNB using 10-fold cross validation\n","\n","from sklearn.naive_bayes import MultinomialNB \n","from sklearn.model_selection import KFold\n","k_folds = KFold(n_splits=10)\n","accuracy = []\n","for train, test in k_folds.split(sp_matrix_train):\n","  train_data = sp_matrix_train[train]\n","  test_data = sp_matrix_train[test]\n","  train_label = df_train_mapped_names_labels['app_label'][train]\n","  test_label = df_train_mapped_names_labels['app_label'][test]\n","  NB = MultinomialNB()\n","  NB.fit(train_data, train_label)\n","  y_pred = NB.predict(test_data)\n","  accuracy.append(sum(np.asarray(test_label) == y_pred) / test_data.shape[0])\n","\n","print('Scikit-learns MultinomialNB 10-fold cross validation accruacy: {:0.2f}%'.format(np.mean(np.asarray(accuracy))*100))\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Scikit-learns MultinomialNB 10-fold cross validation accruacy: 63.23%\n"],"name":"stdout"}]},{"metadata":{"id":"y_xa-3K1iUS_","colab_type":"text"},"cell_type":"markdown","source":["### 3.2 Extensive Analysis\n","\n"]},{"metadata":{"id":"f89yzL6WYDgx","colab_type":"text"},"cell_type":"markdown","source":["The accuracy is a great measure but only when we have symmetric datasets where values of false positive and false negatives are almost same. We computed the performance metrics and got the average precision, recall and F-measure across all class labels as 65.96%, 62.50% and 61.80%. respectively. The values are very similar to each other, and also to our mean accuracy of 63.60%, which shows that the number of false positive and false negative is quite balanced in our dataset. The metrics are suggest that our mean accuracy is a non-biased measure of our classifier. \n","\n","</br>\n","\n","\\begin{equation}\n","    Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}  \n","\\end{equation}\n","\n","</br>\n","\n","\\begin{equation}\n","    Precision (p) = \\frac{TP}{TP+FP}, \n","    Recall (r) = \\frac{TP}{TP+FN},\n","    F\\_measure (F) = \\frac{2rp}{r+p} = \\frac{2TP}{2TP+FN+FP}\n","\\end{equation}\n","\n","</br>\n","\n","\n","We suspect there might be imbalance in the number of samples among class labels so instead of calculating these metrics for each class then macro-averaged them for the entire dataset, we considered micro-averaging the performance metrics by taking the mean of the TP,TN,FP,FN values of each class label, then using these mean values to calculate the overall performance metrics. The results for precision, recall and f-measure are identical, however they are similar to the results we got above when we macro-averaged them. This confirms that number of samples is quite balanced for all class labels.\n","\n","\n","\n","</br>\n","\\begin{equation}\n","Precision_{micro} = \\frac{TP_1 + ... + TP_k  }{TP_1 + ... + TP_k +FP_1 + ... + FP_k }, \n","\\end{equation}\n","</br>\n","\n","</br>\n","\\begin{equation}\n","Precision_{macro} = \\frac{Precision_1 + ... + Precision_k  }{k}\n","\\end{equation}\n","\n","</br>\n","\n"]},{"metadata":{"id":"54NVYHe_Y4UJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"d2da35a2-fed2-496b-95bc-4aee18598657","executionInfo":{"status":"ok","timestamp":1525658909391,"user_tz":-600,"elapsed":10080,"user":{"displayName":"Leslie CHEN","photoUrl":"//lh4.googleusercontent.com/-FXnDbbiS1ww/AAAAAAAAAAI/AAAAAAAAAY4/srCxMVswUdw/s50-c-k-no/photo.jpg","userId":"113849209760449392369"}}},"cell_type":"code","source":["### Obtain training accuracy\n","feature_count = sp_matrix_train.shape[1]\n","sample_size = sp_matrix_train.shape[0]\n","x_train_labels_counts = df_train_mapped_names_labels['app_label'].value_counts()\n","x_train_labels_probabilities = np.log(x_train_labels_counts / df_train_mapped_names_labels['app_label'].size).values\n","classess_feature_probs = multinomial_Bayes_classifier(sp_matrix_train, df_train_mapped_names_labels['app_label'], feature_count)  \n","predicted_result = predict_labels(sp_matrix_train, classess_feature_probs, x_train_labels_counts, x_train_labels_probabilities) \n","print('Training accuracy: {:0.2f}%'.format(sum(np.asarray(df_train_mapped_names_labels['app_label']) == predicted_result) / sample_size * 100))\n"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Training accuracy: 73.73%\n"],"name":"stdout"}]},{"metadata":{"id":"avGE-c9zieOc","colab_type":"text"},"cell_type":"markdown","source":["\n","In order to see if our classifier overfits with the training data, we trained and tested our classifier with the same dataset. The training accuracy we obtained is 73.73%, which is slightly higher than our average test accuracy of 63.60% using 10-fold cross validation. There is a no strong sign to show that our classifier suffers from overfitting.\n","\n"]},{"metadata":{"id":"NHyWvxoDtRdh","colab_type":"text"},"cell_type":"markdown","source":["## 4. Export Results "]},{"metadata":{"id":"QUlCo6QPgTOi","colab_type":"text"},"cell_type":"markdown","source":["#### The 'predicted_labels.csv' file will be exported to the same folder under 'Data': https://drive.google.com/open?id=1OiK2KB7aonQVzKE6dDA0Vnyk3Y190jpi"]},{"metadata":{"id":"ghxmsBWLujkF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":1139},"outputId":"7ee7f457-85a7-45ce-90d2-841dce0d0167","executionInfo":{"status":"ok","timestamp":1525658922404,"user_tz":-600,"elapsed":11425,"user":{"displayName":"Leslie CHEN","photoUrl":"//lh4.googleusercontent.com/-FXnDbbiS1ww/AAAAAAAAAAI/AAAAAAAAAY4/srCxMVswUdw/s50-c-k-no/photo.jpg","userId":"113849209760449392369"}}},"cell_type":"code","source":["# Timing\n","start = timeit.default_timer()\n","\n","### Predict Labels and Save to PyDrive\n","#### Code to predict labels\n","train_labels = df_train_mapped_names_labels['app_label']\n","train_labels_counts = train_labels.value_counts()\n","train_labels_probabilities = np.log(train_labels_counts / train_labels.size).values\n","classess_feature_probs = multinomial_Bayes_classifier(sp_matrix_train, train_labels, feature_count)\n","predicted_labels = predict_labels(sp_matrix_test, classess_feature_probs, train_labels_counts, train_labels_probabilities) \n","predicted_result = pd.concat([df_test_names, pd.DataFrame(predicted_labels, columns=['predicted_labels'])], axis=1)\n","\n","### Generate the predicted_labels.csv to the same Data folder: https://drive.google.com/open?id=1OiK2KB7aonQVzKE6dDA0Vnyk3Y190jpi\n","tgt_folder_id = '1OiK2KB7aonQVzKE6dDA0Vnyk3Y190jpi'\n","parent_folder = {\"kind\": \"drive#fileLink\",\"id\": tgt_folder_id}\n","predicted_labels = drive.CreateFile({'title': 'predicted_labels.csv', 'mimeType':'text/csv', 'parents': [parent_folder]})\n","predicted_labels.SetContentString(predicted_result.to_csv(index=False, header=False))\n","predicted_labels.Upload()\n","print('Created file %s with mimeType %s' % (predicted_labels['title'], predicted_labels['mimeType']))  \n","print(predicted_result)\n","\n","stop = timeit.default_timer()\n","print ('Runtime: {:0.2f} seconds'.format(stop - start ))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Created file predicted_labels.csv with mimeType text/csv\n","                                            app_name     predicted_labels\n","0               dalmax.games.turnBasedGames.connect4     Brain and Puzzle\n","1                       com.holfeld.japaneseplusfree     Travel and Local\n","2                           com.mobileApp.controller      Personalization\n","3                   com.aarontennyson.calorietracker   Health and Fitness\n","4               com.totaldevel.android.todocitas.ads  Books and Reference\n","5                     com.google.android.voicesearch        Communication\n","6                                 fieldbird.yourself            Lifestyle\n","7                          com.dmacattack.mpgConvert       Transportation\n","8                               com.ds.tonsillectomy              Medical\n","9                         metrofax.android.MobileFax         Productivity\n","10                                 sk.halmi.itimerad   Health and Fitness\n","11               com.dreamstep.wPureRomancebyAleacia            Education\n","12                                     org.flinc.app       Transportation\n","13              com.smafundev.android.games.drumspro        Entertainment\n","14                                    com.socialcast             Business\n","15                    com.anpow.mechanicaldictionary  Books and Reference\n","16                             fourandroids.bttoggle                Tools\n","17                      com.lalitasahasranmacafeinks  Books and Reference\n","18                                 com.dictionary.ta  Books and Reference\n","19                        kr.Neosarchizo.EyeTraining   Health and Fitness\n","20    jp.ne.neko.freewing.CameraDeviceMarketTest0000                Tools\n","21                      com.crystalreality.crystaltv      Media and Video\n","22                                 ro.headlight.RTUI             Shopping\n","23                            com.painone7.Solitaire     Cards and Casino\n","24                               loyalblocks.userapp             Shopping\n","25                               com.synaptik.blocks     Brain and Puzzle\n","26                               com.juteralabs.gush             Shopping\n","27                                 com.dictionary.te  Books and Reference\n","28              com.appmk.magazine.AOTNXENBDOVARBFMJ  Books and Reference\n","29                                br.eti.fml.satoshi              Finance\n","...                                              ...                  ...\n","2203                        com.piviandco.agingbooth        Entertainment\n","2204                           com.alpha_aps.plumber     Brain and Puzzle\n","2205                      com.nousguide.android.rbtv               Sports\n","2206                      iec.mycatphotosticker.free          Photography\n","2207                          com.kiaigames.Sprinter               Racing\n","2208                        com.saaranga.wikikannada  Books and Reference\n","2209                      com.decas.BuddhismRingtone      Music and Audio\n","2210                com.pbs.android.animalpuzzlefree            Education\n","2211                    com.kodak.wmc.rsscombinedapp          Photography\n","2212                           vn.esse.bodysymbol.hd      Personalization\n","2213              com.killermobile.totalrecall.trial        Communication\n","2214               com.aqreadd.lw.clockdown.lite.gle      Personalization\n","2215                          org.apache.medicalterm              Medical\n","2216                            com.hardest.ballgame     Brain and Puzzle\n","2217                            com.thomson.druginfo              Medical\n","2218                  com.interactionstudios.CI4.ggl    Arcade and Action\n","2219                              com.dubai.metromap       Transportation\n","2220            com.djinnworks.StickStuntBiker2.free               Racing\n","2221                                 de.PostkartePro             Shopping\n","2222        com.samsung.dct.retailagent.phthree.test   Libraries and Demo\n","2223              com.henryh.android.supermarketlite             Shopping\n","2224                  app.riseset.sunrise.and.sunset              Weather\n","2225              com.Audio_Therapy_Binaural_Beats_2   Health and Fitness\n","2226                           com.vivamedia.cmGGTHD     Brain and Puzzle\n","2227                              de.mamru.intervals   Health and Fitness\n","2228                               sg.com.squarefoot             Business\n","2229                           kr.co.yam3.searchmenu     Travel and Local\n","2230              com.daeha.android.app.repeat_ja_ko  Books and Reference\n","2231                 com.sec.android.allShareControl      Media and Video\n","2232       com.mobileriders.butt.sculpting.exercises   Health and Fitness\n","\n","[2233 rows x 2 columns]\n","Runtime: 10.17 seconds\n"],"name":"stdout"}]},{"metadata":{"id":"IEl4Jjo6ifu1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"70666a6c-b1f1-470e-bfc7-da917eb483ad","executionInfo":{"status":"ok","timestamp":1525658930022,"user_tz":-600,"elapsed":504,"user":{"displayName":"Leslie CHEN","photoUrl":"//lh4.googleusercontent.com/-FXnDbbiS1ww/AAAAAAAAAAI/AAAAAAAAAY4/srCxMVswUdw/s50-c-k-no/photo.jpg","userId":"113849209760449392369"}}},"cell_type":"code","source":["print('Memory usage for this process so far is: ' + str(memory_usage_psutil()) + ' MB')"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Memory usage for this process so far is: 5705.1171875 MB\n"],"name":"stdout"}]},{"metadata":{"id":"CBYRlGaCbknL","colab_type":"text"},"cell_type":"markdown","source":["**Insert the url address of your predicted_labels.csv file:   **\n","\n"]},{"metadata":{"id":"F-Tt-R-l5Isa","colab_type":"text"},"cell_type":"markdown","source":["https://drive.google.com/open?id=1-GqSDX8aA0vUXMtl6En5lH0W4w4sU_5h"]},{"metadata":{"id":"z9tuqCpZigo1","colab_type":"text"},"cell_type":"markdown","source":["## 5. Discussion "]},{"metadata":{"id":"skiNNJW1ikjg","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","\n"]},{"metadata":{"id":"NXwLgIecsBBK","colab_type":"text"},"cell_type":"markdown","source":["### 5.1\tRuntime and Memory Usage\n","\n","Our classifier takes approximately 85 seconds to run the 10-fold cross validation on our training dataset, and 11 seconds to predict the labels for our test dataset. After we run all the codes, the total memory usage is roughly 4658 MB. Hardware and software specifications of our local computer: \n","MacOS High Sierra, Processor: 2.7 GHz Intel Core i7, Memory: 16 GB, Graphics: Radeon Pro 455 2048 MB, Intel HD Graphics 530 1536 MB. IDE: Pycharm.\n","\n","\n","\n","### 5.2\tProblem with Naïve Bayes classifier\n","\n","The major systemic problem with Naive Bayes approach is that the independent assumption of all the feature is drastic and rarely true. As each app has 13,626 features, there is a high probability that some of the features has covariate effect with each other and the assumption of independent features may fail to hold. Furthermore, there is no feature name in the dataset, the uncertainty about the feature exacerbated the problem of independent assumption. Another problem with Naive Bayes Classifier is that the requirement of balanced training samples.  The app's data that corresponding to these 30 classes are relatively balanced, but not perfectly balance. In this case, the unbalanced effect under different labels may warn us that if the class has more training examples than another, Naive Bayes classifier may select poor weights for the decision boundary. This is due to an understudied bias effect that shrinks weights for classes with few training examples (Rennie et.al., 2016). However, by using both micro-average and macro-average to compute and compare the performance metrics (recall, precision, f-measure) in our 10-fold cross validation, we are positive that the number of samples among 30 class labels in each iteration is quite balanced.\n","\n","\n","\n","### 5.3 The drawback of the generative approach \n","\n","In general, the generative model is not good at optimizing classification accuracy. As a generative model, Naive Bayes approach aims for a complete probabilistic description of the data by constructing the joint probability distribution. However, our main focus is to optimize classification accuracy for Apps instead of making a solid claim about how model parameters interact. If we apply a discriminative model like SVM and logistic regression, the accuracy may be higher. \n","\n"]},{"metadata":{"id":"kViLL0fEirga","colab_type":"text"},"cell_type":"markdown","source":["## 6. Conclusion and future work \n","\n"]},{"metadata":{"id":"QP7ckuJxjCqh","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","\n","\n","---\n","\n"]},{"metadata":{"id":"Wv-7SL2-sMNT","colab_type":"text"},"cell_type":"markdown","source":["By considering there will be more apps coming into the App market every day, we believe that Naïve Bayes classifier is stable and able to adapt quickly to the changes of dynamic data. Even though the “naïve” assumption of independence among features are hardly true, but in practice, Naive Bayes models have performed surprisingly well, even on complex tasks where the strong independence assumptions are false. Basically, some of the reasons the Naïve Bayes classifier is so common to use is that it is fast, easy to implement and relatively effective to undertake the classification tasks. However, by considering the size of feature and classes, it may not always work well. For performance evaluation, it is important to consider the class distribution and the costs of each misclassification in each class. The future work of this problem could be discussed by introducing incremental learning algorithm. Also, to improve the classification accuracy, we might consider non-linear classifiers and other discriminative models such SVM and logistic regression."]},{"metadata":{"id":"il1S9xh5suYH","colab_type":"text"},"cell_type":"markdown","source":["## 7. References"]},{"metadata":{"id":"bW7XuBcWs0x2","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n"]},{"metadata":{"id":"tB9UVjbwtCRW","colab_type":"text"},"cell_type":"markdown","source":["\n","1.   Pandas.read_csv. (n.d.). Retrieved 25 April 2018 from https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n","2.   DataQuest. (2017). Using pandas with large data. Retrieved 25 April 2018 from https://www.dataquest.io/blog/pandas-big-data/\n","3.   Stackoverflow. (2014). Retrieved 25 April 2018 from https://stackoverflow.com/questions/22934525/pydrive-cannot-write-file-to-specific-gdrive-folder\n","4.   Stuart, J., Russell, P and Norvig, P. (2003). *Artificial Intelligence: A Modern Approach (2 ed.). Pearson Education.* See p. 499 for reference to \"idiot Bayes\" as well as the general definition of the Naive Bayes model and its independence assumptions\n","5. Domingos, P., & Pazzani, M. (1996). Beyond independence: conditions for the optimality of the simple Bayesian classifier. Proceedings of ICML ’96. \n","\n","6. McCallum, A., & Nigam, K. (1998). *A comparison of event models for naive Bayes text classification. Proceedings of AAAI* . \n","7. Rennie, J., Shih, L. & Teevan, J. (2003). *Tackling the Poor Assumptions of Naive Bayes Text Classifiers*.Artificial Intelligence Laboratory, Massachusetts Institute of Technology\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"DzqTre2yu6YJ","colab_type":"text"},"cell_type":"markdown","source":["# Sumbitting your assignment"]},{"metadata":{"id":"mwUw2n7uu9zH","colab_type":"text"},"cell_type":"markdown","source":["You must share only 1 folder with your tutor.\n","The folder should have your group's unikeys in its name ( example: Assignment1_abxy6273_edfr3373_yhfr4534 ).\n","\n","The folder should contain a Colab notebook and the output file.\n","\n","Suggested structure of the folder:\n","\n","* Data\n"," * training_data.csv\n"," * training_desc.csv\n"," * training_labels.csv\n"," * test_data.csv\n","* Assignment1.ipynb\n","* predicted_labels.csv\n","\n","Ask your tutor for his email address and share the properly named folder with him before 5:00pm on May 7th 2018. \n","Every late day will cost you 20 marks.\n","\n"]}]}